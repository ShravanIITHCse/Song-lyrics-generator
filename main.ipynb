{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4PBauYokGrD"
      },
      "source": [
        "# Song Lyrics Generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zxBzugekOsJ",
        "outputId": "e554a0c7-f53d-4024-c4d4-e94ba0ae0fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "HmT8C4EckGrO"
      },
      "source": [
        "## Data Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6GiQXRa6kGrQ"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3yl1m3RkGrS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpV8NeW2kGrW"
      },
      "source": [
        "### Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujT72ArCkGrX",
        "outputId": "3966fa1d-2f4c-47b9-a101-30573d798ef6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song_id</th>\n",
              "      <th>lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n",
              "      <td>[\"[Verse 1]\\nFound you when your heart was bro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n",
              "      <td>['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1rqqCSm0Qe4I9rUvWncaom</td>\n",
              "      <td>[\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0bYg9bo50gSsH3LtXe2SQn</td>\n",
              "      <td>[\"[Intro]\\nI-I-I don't want a lot for Christma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5hslUAKq9I9CG2bAulFkHN</td>\n",
              "      <td>['[Chorus]\\nIt\\'s the most wonderful time of t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  song_id                                             lyrics\n",
              "1  5p7ujcrUXASCNwRaWNHR1C  [\"[Verse 1]\\nFound you when your heart was bro...\n",
              "2  2xLMifQCjDGFmkHkpNLD9h  ['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...\n",
              "4  1rqqCSm0Qe4I9rUvWncaom  [\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...\n",
              "5  0bYg9bo50gSsH3LtXe2SQn  [\"[Intro]\\nI-I-I don't want a lot for Christma...\n",
              "6  5hslUAKq9I9CG2bAulFkHN  ['[Chorus]\\nIt\\'s the most wonderful time of t..."
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('./data/lyrics.csv', sep = \"\\t\")\n",
        "df = df.dropna(subset=['lyrics'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp343BhVkGra",
        "outputId": "9f454c66-5e74-4a5e-941d-c4da099744a4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Poem</th>\n",
              "      <th>Poet</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\r\\n\\r\\n                    Objects Used to Pr...</td>\n",
              "      <td>\\r\\n\\r\\nDog bone, stapler,\\r\\n\\r\\ncribbage boa...</td>\n",
              "      <td>Michelle Menting</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\r\\n\\r\\n                    The New Church\\r\\n...</td>\n",
              "      <td>\\r\\n\\r\\nThe old cupola glinted above the cloud...</td>\n",
              "      <td>Lucia Cherciu</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\r\\n\\r\\n                    Look for Me\\r\\n\\r\\...</td>\n",
              "      <td>\\r\\n\\r\\nLook for me under the hood\\r\\n\\r\\nof t...</td>\n",
              "      <td>Ted Kooser</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\r\\n\\r\\n                    Wild Life\\r\\n\\r\\n ...</td>\n",
              "      <td>\\r\\n\\r\\nBehind the silo, the Mother Rabbit\\r\\n...</td>\n",
              "      <td>Grace Cavalieri</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\r\\n\\r\\n                    Umbrella\\r\\n\\r\\n  ...</td>\n",
              "      <td>\\r\\n\\r\\nWhen I push your button\\r\\n\\r\\nyou fly...</td>\n",
              "      <td>Connie Wanek</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  \\\n",
              "0  \\r\\n\\r\\n                    Objects Used to Pr...   \n",
              "1  \\r\\n\\r\\n                    The New Church\\r\\n...   \n",
              "2  \\r\\n\\r\\n                    Look for Me\\r\\n\\r\\...   \n",
              "3  \\r\\n\\r\\n                    Wild Life\\r\\n\\r\\n ...   \n",
              "4  \\r\\n\\r\\n                    Umbrella\\r\\n\\r\\n  ...   \n",
              "\n",
              "                                                Poem              Poet Tags  \n",
              "0  \\r\\n\\r\\nDog bone, stapler,\\r\\n\\r\\ncribbage boa...  Michelle Menting  NaN  \n",
              "1  \\r\\n\\r\\nThe old cupola glinted above the cloud...     Lucia Cherciu  NaN  \n",
              "2  \\r\\n\\r\\nLook for me under the hood\\r\\n\\r\\nof t...        Ted Kooser  NaN  \n",
              "3  \\r\\n\\r\\nBehind the silo, the Mother Rabbit\\r\\n...   Grace Cavalieri  NaN  \n",
              "4  \\r\\n\\r\\nWhen I push your button\\r\\n\\r\\nyou fly...      Connie Wanek  NaN  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf = pd.read_csv('./data/PoetryFoundationData.csv', quotechar='\"')\n",
        "pdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8lSxcGkGrc"
      },
      "source": [
        "## Cleansing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DFoECAJkGrd"
      },
      "outputs": [],
      "source": [
        "# This translator is used to remove punctuation from the text\n",
        "translator = str.maketrans('', '', string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih2czC-MkGre"
      },
      "source": [
        "### Cleansing Lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOeqtAm-kGrf"
      },
      "outputs": [],
      "source": [
        "def split_text(x):\n",
        "    text = x['lyrics'] # get the lyrics\n",
        "#    print(text)\n",
        "    sections = text.split('\\\\n\\\\n') # split the lyrics into sections\n",
        "    # for s in sections:\n",
        "        # print(s)\n",
        "    keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan, 'Intro': np.nan}\n",
        "    lyrics = str()\n",
        "    single_text = []\n",
        "    res = {}\n",
        "    for s in sections:\n",
        "        key = s[s.find('[') + 1:s.find(']')].strip()\n",
        "        if len(key)>3 and key[0] == '\"':\n",
        "            key = key[2:]\n",
        "        if ':' in key:\n",
        "           key = key[:key.find(':')]\n",
        "        # print(key)\n",
        "        if key in keys:\n",
        "           single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n",
        "\n",
        "    # print(single_text)\n",
        "\n",
        "        res['single_text'] =  ' \\n '.join(single_text)\n",
        "    # print(res)\n",
        "    return pd.Series(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-ggA3KhkGrg",
        "outputId": "1a37b819-e31a-419b-e438-e51ccfab098b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song_id</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>single_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n",
              "      <td>[\"[Verse 1]\\nFound you when your heart was bro...</td>\n",
              "      <td>found you when your heart was broke \\n i fille...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n",
              "      <td>['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...</td>\n",
              "      <td>astro yeah \\n sun is down freezin cold \\n that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1rqqCSm0Qe4I9rUvWncaom</td>\n",
              "      <td>[\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...</td>\n",
              "      <td>high high hopes \\n had to have high high hopes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0bYg9bo50gSsH3LtXe2SQn</td>\n",
              "      <td>[\"[Intro]\\nI-I-I don't want a lot for Christma...</td>\n",
              "      <td>iii dont want a lot for christmas \\n there is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5hslUAKq9I9CG2bAulFkHN</td>\n",
              "      <td>['[Chorus]\\nIt\\'s the most wonderful time of t...</td>\n",
              "      <td>its the haphappiest season of all \\n with thos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  song_id                                             lyrics  \\\n",
              "1  5p7ujcrUXASCNwRaWNHR1C  [\"[Verse 1]\\nFound you when your heart was bro...   \n",
              "2  2xLMifQCjDGFmkHkpNLD9h  ['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...   \n",
              "4  1rqqCSm0Qe4I9rUvWncaom  [\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...   \n",
              "5  0bYg9bo50gSsH3LtXe2SQn  [\"[Intro]\\nI-I-I don't want a lot for Christma...   \n",
              "6  5hslUAKq9I9CG2bAulFkHN  ['[Chorus]\\nIt\\'s the most wonderful time of t...   \n",
              "\n",
              "                                         single_text  \n",
              "1  found you when your heart was broke \\n i fille...  \n",
              "2  astro yeah \\n sun is down freezin cold \\n that...  \n",
              "4  high high hopes \\n had to have high high hopes...  \n",
              "5  iii dont want a lot for christmas \\n there is ...  \n",
              "6  its the haphappiest season of all \\n with thos...  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.join( df.apply(split_text, axis=1))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqg3FnjXkGrh",
        "outputId": "ace32a0a-b005-4443-9a1a-268023825592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19662, 3)\n",
            "(19662, 3)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)\n",
        "df.dropna(subset=['single_text'], inplace=True)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaS4s2EJkGrh"
      },
      "source": [
        "#### Saving the cleansed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOxxERjmkGri"
      },
      "outputs": [],
      "source": [
        "df.to_csv('./data/lyrics_clean.csv', sep = \"\\t\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUZMNysAkGri"
      },
      "source": [
        "#### Loading the cleansed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgNJFPYkGri"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/lyrics_clean.csv', sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahSLVF1NkGrj"
      },
      "source": [
        "### Cleansing Poems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra9rstirkGrj",
        "outputId": "861af436-cb59-4d0b-f526-9d741cba45de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    Dog bone, stapler,\\ncribbage board, garlic pre...\n",
            "0    At the high school football game, the boys\\nst...\n",
            "0                                               #1 ...\n",
            "0    The truth is, I’ve never cared for the Nationa...\n",
            "0    Part of suffering is the useless urge to annou...\n",
            "                           ...                        \n",
            "0    They eat beans mostly, this old yellow pair.  ...\n",
            "0    The accumulation of reefs\\npiling up one over ...\n",
            "0              Philosophic\\nin its complex, ovoid e...\n",
            "0                                                     \n",
            "0              Philosophic\\nin its complex, ovoid e...\n",
            "Name: Poem, Length: 100, dtype: object\n",
            "0    dog bone stapler \\n cribbage board garlic pres...\n",
            "0    at the high school football game the boys \\n s...\n",
            "0    1 college \\n  \\n we packed your satchel with s...\n",
            "0    the truth is i’ve never cared for the national...\n",
            "0    part of suffering is the useless urge to annou...\n",
            "                           ...                        \n",
            "0    they eat beans mostly this old yellow pair \\n ...\n",
            "0    the accumulation of reefs \\n piling up one ove...\n",
            "0    philosophic \\n in its complex ovoid emptiness ...\n",
            "0                                                     \n",
            "0    philosophic \\n in its complex ovoid emptiness ...\n",
            "Name: single_text, Length: 100, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# remove /r from the text\n",
        "pdf['Poem'] = pdf['Poem'].apply(lambda x: x.replace('\\r', ''))\n",
        "# replace \\n\\n with \\n\n",
        "pdf['Poem'] = pdf['Poem'].apply(lambda x: x.replace('\\n\\n', '\\n'))\n",
        "pdf['Poem'] = pdf['Poem'].apply(lambda x: x.replace('\\t', '\\n'))\n",
        "# remove first \\n\n",
        "pdf['Poem'] = pdf['Poem'].apply(lambda x: x[1:] if x[0] == '\\n' else x)\n",
        "# remove lines with less than 2 characters\n",
        "# pdf['Poem'] = pdf['Poem'].apply(lambda x: '\\n'.join([l for l in x.splitlines() if len(l)>1]))\n",
        "pdf['single_text'] = pdf['Poem'].apply(lambda x: ' \\n '.join([l.lower().strip().translate(translator) for l in x.splitlines() if len(l)>0]))\n",
        "# pdf.head()\n",
        "print(pdf['Poem'][0])\n",
        "print(pdf['single_text'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01nAd5jVkGrk"
      },
      "source": [
        "### Combinind the cleansed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_kTryZ5kGrk"
      },
      "outputs": [],
      "source": [
        "sum_df = pd.DataFrame(df['single_text'])\n",
        "# sum_df = pd.concat([sum_df, pd.DataFrame(pdf['single_text'])], ignore_index=True)\n",
        "sum_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1pOHPFXkGrk"
      },
      "source": [
        "#### Saving the combined data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nTKaEWekGrl"
      },
      "outputs": [],
      "source": [
        "sum_df.to_csv('./data/sum_data.csv', sep = \"\\t\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa_GXRvIkGrl"
      },
      "source": [
        "#### Loading the combined data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8sEDDs3kGrl"
      },
      "outputs": [],
      "source": [
        "sum_df = pd.read_csv('./data/sum_data.csv', sep = \"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6RaexNikGrl",
        "outputId": "f3a22856-cf30-4744-fdbc-6dc95ec8f4ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>single_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>found you when your heart was broke \\n i fille...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>astro yeah \\n sun is down freezin cold \\n that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>high high hopes \\n had to have high high hopes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>iii dont want a lot for christmas \\n there is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>its the haphappiest season of all \\n with thos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         single_text\n",
              "0  found you when your heart was broke \\n i fille...\n",
              "1  astro yeah \\n sun is down freezin cold \\n that...\n",
              "2  high high hopes \\n had to have high high hopes...\n",
              "3  iii dont want a lot for christmas \\n there is ...\n",
              "4  its the haphappiest season of all \\n with thos..."
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEVKqzuskGrm"
      },
      "source": [
        "## Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VJUNx_CkGrm"
      },
      "source": [
        "### Finding the unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFdGRzrokGrm",
        "outputId": "8e95585d-32f9-4bff-fe17-a1355e28b2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words  2634181\n"
          ]
        }
      ],
      "source": [
        "text_as_list = []\n",
        "\n",
        "def extract_text(text):\n",
        "   global text_as_list\n",
        "   text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "\n",
        "sum_df['single_text'].apply(extract_text)\n",
        "\n",
        "print(\"Total words \" , len(text_as_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAbJ00ANkGrm",
        "outputId": "757184f1-298a-4415-88d3-708b3739d93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['til', 'i', 'land', '\\n', 'had', 'me', 'out', 'like', 'a', 'light', 'like', 'a', 'light', '\\n', 'like', 'a', 'light', 'like', 'a', 'light', '\\n', 'like', 'a', 'light', 'like', 'a', 'light', '\\n', 'like', 'a', 'light', '\\n', 'yeah', 'passed', 'the', 'dawgs', 'a', 'celly', '\\n', 'sendin', 'texts', 'aint', 'sendin', 'kites', 'yeah', '\\n', 'he', 'said', 'keep', 'that', 'on', 'lock', '\\n', 'i', 'say', 'you', 'know', 'this', 'shit', 'its', 'stife', 'yeah', '\\n', 'its', 'absolute', 'yeah', 'yeah', 'im', 'back', 'reboot', 'its', 'lit', '\\n', 'laferrari', 'to', 'jamba', 'juice', 'yeah', 'skrrt', 'skrrt', '\\n', 'we', 'back', 'on', 'the', 'road', 'they', 'jumpin', 'off', 'no', 'parachute', 'yeah', '\\n', 'shawty', 'in', 'the', 'back', '\\n', 'she', 'said', 'she', 'workin', 'on', 'her', 'glutes', 'yeah', 'oh', 'my', 'god', '\\n', 'aint', 'by', 'the', 'book', 'yeah', 'this', 'how', 'it', 'look', 'yeah', '\\n', 'bout', 'a', 'check', 'yeah', 'check', 'just', 'check', 'the', 'foots', 'yeah', '\\n', 'pass', 'this', 'to', 'my', 'daughter', 'ima', 'show', 'her', 'what', 'it', 'took', 'yeah', '\\n', 'baby', 'mama', 'cover', 'forbes', 'got', 'these', 'other', 'bitches', 'shook', '\\n', 'yeah', 'high', 'high', 'hopes', '\\n', 'had', 'to', 'have', 'high', 'high', 'hopes', 'for', 'a', 'living', '\\n', 'shooting', 'for', 'the', 'stars', 'when', 'i', 'couldnt', 'make', 'a', 'killing', '\\n', 'didnt', 'have', 'a', 'dime', 'but', 'i', 'always', 'had', 'a', 'vision', '\\n', 'always', 'had', 'high', 'high', 'hopes', 'high', 'high', 'hopes', '\\n', 'had', 'to', 'have', 'high', 'high', 'hopes', 'for', 'a', 'living', '\\n', 'didnt', 'know', 'how', 'but', 'i', 'always', 'had', 'a', 'feeling', '\\n', 'i', 'was', 'gonna', 'be', 'that', 'one', 'in', 'a', 'million', '\\n', 'always', 'had', 'high', 'high', 'hopes', '\\n', 'mama', 'said', 'fulfill', 'the', 'prophecy', '\\n', 'be', 'something', 'greater', 'go', 'make', 'a', 'legacy', '\\n', 'manifest', 'destiny', 'back', 'in', 'the', 'days', '\\n', 'we', 'wanted', 'everything', 'wanted', 'everything', '\\n', 'mama', 'said', 'burn', 'your', 'biographies', '\\n', 'rewrite', 'your', 'history', 'light', 'up', 'your', 'wildest', 'dreams', '\\n', 'museum', 'victories', 'every', 'day', '\\n', 'we', 'wanted', 'everything', 'wanted', 'everything', '\\n', 'had', 'to', 'have', 'high', 'high', 'hopes', 'for', 'a', 'living', '\\n', 'shooting', 'for', 'the', 'stars', 'when', 'i', 'couldnt', 'make', 'a', 'killing', '\\n', 'didnt', 'have', 'a', 'dime', 'but', 'i', 'always', 'had', 'a', 'vision', '\\n', 'always', 'had', 'high', 'high', 'hopes', '\\n', 'had', 'to', 'have', 'high', 'high', 'hopes', 'for', 'a', 'living', '\\n', 'didnt', 'know', 'how', 'but', 'i', 'always', 'had', 'a', 'feeling', '\\n', 'i', 'was', 'gonna', 'be', 'that', 'one', 'in', 'a', 'million', '\\n', 'always', 'had', 'high', 'high', 'hopes', 'high', 'high', 'hopes', '\\n', 'mama', 'said', 'its', 'uphill', 'for', 'oddities', '\\n', 'the', 'stranger', 'crusaders', 'aint', 'ever', 'wannabes', '\\n', 'the', 'weird', 'and', 'the', 'novelties', 'dont', 'ever', 'change', '\\n', 'we', 'wanted', 'everything', 'wanted', 'everything', 'high', 'high', 'hopes', '\\n', 'had', 'to', 'have', 'high', 'high', 'hopes', 'for', 'a', 'living', '\\n', 'shooting', 'for', 'the', 'stars', 'when', 'i', 'couldnt', 'make', 'a', 'killing', '\\n', 'didnt', 'have', 'a', 'dime', 'but', 'i', 'always', 'had', 'a', 'vision', '\\n', 'always', 'had', 'high', 'high', 'hopes', '\\n', 'had', 'to', 'have', 'high', 'high', 'hopes', 'for', 'a', 'living', '\\n', 'didnt', 'know', 'how', 'but', 'i', 'always', 'had', 'a', 'feeling', '\\n', 'i', 'was', 'gonna', 'be', 'that', 'one', 'in', 'a', 'million', '\\n', 'always', 'had', 'high', 'high', 'hopes', 'high', 'high', 'hopes', 'iii', 'dont', 'want', 'a', 'lot', 'for', 'christmas', '\\n', 'there', 'is', 'just', 'one', 'thing', 'i', 'need', '\\n', 'i', 'dont', 'care', 'about', 'the', 'presents', '\\n', 'underneath', 'the', 'christmas', 'tree', '\\n', 'i', 'just', 'want', 'you', 'for', 'my', 'own', '\\n', 'more', 'than', 'you', 'could', 'ever', 'know', '\\n', 'make', 'my', 'wish', 'come', 'true', '\\n', 'all', 'i', 'want', 'for', 'christmas', 'is', 'you', 'yeah', '\\n', 'i', 'dont', 'want', 'a', 'lot', 'for', 'christmas', '\\n', 'there', 'is', 'just', 'one', 'thing', 'i', 'need', 'and', 'i', '\\n', 'dont', 'care', 'about', 'the', 'presents', '\\n', 'underneath', 'the', 'christmas', 'tree', '\\n', 'i', 'dont', 'need', 'to', 'hang', 'my', 'stocking', '\\n', 'there', 'upon', 'the', 'fireplace', 'i', '\\n', 'santa', 'claus', 'wont', 'make', 'me', 'happy', '\\n', 'with', 'a', 'toy', 'on', 'christmas', 'day', '\\n', 'i', 'just', 'want', 'you', 'for', 'my', 'own', 'ooh', 'ooh', 'ooh', '\\n', 'more', 'than', 'you', 'could', 'ever', 'know', 'oh', 'oh', 'oh', '\\n', 'make', 'my', 'wish', 'come', 'true', '\\n', 'all', 'i', 'want', 'for', 'christmas', 'is', 'you', '\\n', 'you', 'baby', '\\n', 'oh', 'i', 'wont', 'ask', 'for', 'much', 'this', 'christmas', '\\n', 'i', 'wont', 'even', 'wish', 'for', 'snow', 'and', 'i', '\\n', 'im', 'just', 'going', 'to', 'keep', 'on', 'waiting', '\\n', 'underneath', 'the', 'mistletoe', '\\n', 'i', 'wont', 'make', 'a', 'list', 'and', 'send', 'it', '\\n', 'to', 'the', 'north', 'pole', 'for', 'saint', 'nick', 'i', '\\n', 'i', 'wont', 'even', 'stay', 'awake', 'to', '\\n', 'hear', 'those', 'magic', 'reindeer', 'click', '\\n', 'cause', 'i', 'just', 'want', 'you', 'here', 'tonight', 'ooh', 'ooh', 'ooh', '\\n', 'holding', 'on', 'to', 'me', 'so', 'tight', 'oh', 'oh', 'oh', '\\n', 'what', 'more', 'can', 'i', 'do', '\\n', 'oh', 'baby', 'all', 'i', 'want', 'for', 'christmas', 'is', 'you', '\\n', 'you', 'baby', '\\n', 'oh', 'i', 'dont', 'want', 'a', 'lot', 'for', 'christmas', '\\n', 'this', 'is', 'all', 'im', 'asking', 'for', 'ah', '\\n', 'i', 'just', 'want', 'to', 'see', 'my', 'baby', '\\n', 'standing', 'right', 'outside', 'my', 'door', '\\n', 'oh', 'i', 'just', 'want', 'you', 'for', 'my', 'own', 'ooh', '\\n', 'more', 'than', 'you', 'could', 'ever', 'know', 'oh', 'oh', '\\n', 'make', 'my', 'wish', 'come', 'true', '\\n', 'oh', 'baby', 'all', 'i', 'want', 'for', 'christmas', 'is', 'its', 'the', 'haphappiest', 'season', 'of', 'all', '\\n', 'with', 'those', 'holiday', 'greetings', '\\n', 'and', 'gay', 'happy', 'meetings', 'when', 'friends', 'come', 'to', 'call', '\\n', 'its', 'the', 'haphappiest', 'season', 'of', 'all', '\\n', 'its', 'the', 'most', 'wonderful', 'time', 'of', 'the', 'year', '\\n', 'therell', 'be', 'much', 'mistletoeing', '\\n', 'and', 'hearts', 'will', 'be', 'glowing', 'when', 'loved', 'ones', 'are', 'near', '\\n', 'its', 'the', 'most', 'wonderful', 'time', 'of', 'the', 'year', '\\n', 'its', 'the', 'most', 'wonderful', 'time', 'of', 'the', 'year', '\\n', 'therell', 'be', 'much', 'mistletoeing', '\\n', 'and', 'hearts', 'will', 'be', 'glowing', 'when', 'loved', 'ones', 'are', 'near', '\\n', 'its', 'the', 'most', 'wonderful', 'time', '\\n', 'its', 'the', 'most', 'wonderful', 'time', '\\n', 'its', 'the', 'most', 'wonderful', 'time', '\\n', 'its', 'the', 'most', 'wonderful', 'time', 'of', 'the', 'year', 'rockin', 'around', 'the', 'christmas', 'tree', '\\n', 'have', 'a', 'happy', 'holiday', '\\n', 'everyones', 'dancing', 'merrily', '\\n', 'in', 'a', 'new', 'old', 'fashioned', 'way', '\\n', 'rockin', 'around', 'the', 'christmas', 'tree', '\\n', 'have', 'a', 'happy', 'holiday', '\\n', 'everyones', 'dancing', 'merrily', '\\n', 'in', 'a', 'new', 'old', 'fashioned', 'way', '16yrold', '\\n', 'bitch', '\\n', 'daytrip', 'took', 'it', 'to', '10', '\\n', 'i', 'got', 'hoes', '\\n', 'callin', 'a', 'young', 'nigga', 'phone', 'ring', 'ring', '\\n', 'wheres', 'ali', 'with', 'the', 'mothafuckin', 'dope', 'bitch', 'bitch', '\\n', 'i', 'be', 'ballin', 'like', 'a', 'mothafuckin', 'pro', 'like', 'a', 'huh', 'like', 'a', 'huh', '\\n', 'i', 'be', 'ballin', 'like', 'my']\n"
          ]
        }
      ],
      "source": [
        "print(text_as_list[1000:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atPhOq-akGrn",
        "outputId": "fb3a4907-671c-4c10-a6f1-e7236365ce7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Words  37659\n"
          ]
        }
      ],
      "source": [
        "freq = {}\n",
        "\n",
        "for w in text_as_list:\n",
        "    if w in freq:\n",
        "        freq[w] += 1\n",
        "    else:\n",
        "        freq[w] = 1\n",
        "\n",
        "print(\"Unique Words \" , len(freq))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiTycY3skGrn"
      },
      "source": [
        "#### Finding Common Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F85NCXUkGro",
        "outputId": "9f66aedd-4993-474d-f3b2-d2d3f801b9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uncommon Words  27953\n",
            "Common Words  9706\n"
          ]
        }
      ],
      "source": [
        "uncommon_words = set([key for key in freq.keys() if freq[key] < 7]) # thala for a reason\n",
        "words = sorted(set([key for key in freq.keys() if freq[key] >= 7])) # thala for a reason\n",
        "\n",
        "print(\"Uncommon Words \" , len(uncommon_words))\n",
        "print(\"Common Words \" , len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpOuEHZIkGro"
      },
      "outputs": [],
      "source": [
        "word_indices = dict((w, i) for i, w in enumerate(words))\n",
        "indices_word = dict((i, w) for i, w in enumerate(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMrokJFXkGrp"
      },
      "source": [
        "#### Forming the sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL03JkPMkGrp"
      },
      "outputs": [],
      "source": [
        "MIN_SEQ = 5\n",
        "\n",
        "valid_seqs = []\n",
        "end_seq_words = []\n",
        "for i in range(len(text_as_list) - MIN_SEQ ):\n",
        "   end_slice = i + MIN_SEQ + 1\n",
        "   if len( set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
        "       valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
        "       end_seq_words.append(text_as_list[i + MIN_SEQ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrswLjezkGrp",
        "outputId": "60f8b721-26ca-494e-ea9d-1aee83615924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid Sequences  2366419\n",
            "End Words  2366419\n",
            "['found', 'you', 'when', 'your', 'heart']   was\n",
            "['you', 'when', 'your', 'heart', 'was']   broke\n",
            "['when', 'your', 'heart', 'was', 'broke']   \n",
            "\n",
            "['your', 'heart', 'was', 'broke', '\\n']   i\n",
            "['heart', 'was', 'broke', '\\n', 'i']   filled\n",
            "['was', 'broke', '\\n', 'i', 'filled']   your\n",
            "['broke', '\\n', 'i', 'filled', 'your']   cup\n",
            "['\\n', 'i', 'filled', 'your', 'cup']   until\n",
            "['i', 'filled', 'your', 'cup', 'until']   it\n",
            "['\\n', 'took', 'it', 'so', 'far']   to\n"
          ]
        }
      ],
      "source": [
        "print(\"Valid Sequences \" , len(valid_seqs))\n",
        "print(\"End Words \" , len(end_seq_words))\n",
        "\n",
        "for i in range(10):\n",
        "    print(valid_seqs[i], \" \", end_seq_words[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AIAv6tJkGrp"
      },
      "source": [
        "### Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6650o75ikGrq"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4odxD10DkGr5"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYcCioRmkGr6",
        "outputId": "181890b7-39df-4d83-fce0-499302a1a73f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Size  2319090\n",
            "Test Size  47329\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Size \", len(X_train))\n",
        "print(\"Test Size \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCUeoafnkGr7"
      },
      "source": [
        "#### Save the split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk0MqR3-kGr7"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "wFhmXdunkGr8",
        "outputId": "2cef8c81-bcba-4499-ed01-62ab0a25d913"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-061290e3e7b1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X_test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_indices'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'indices_word'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindices_word\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test, 'word_indices': word_indices, 'indices_word': indices_word}\n",
        "\n",
        "with open('./data/data.pkl', 'wb') as f:\n",
        "    pickle.dump(data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x8XdYtWkGr8"
      },
      "source": [
        "#### Load the split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvIXWFVmkGr8"
      },
      "outputs": [],
      "source": [
        "data = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/data.pkl', 'rb'))\n",
        "\n",
        "X_train = data['X_train']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']\n",
        "word_indices = data['word_indices']\n",
        "indices_word = data['indices_word']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2VGY3WXkGr9"
      },
      "source": [
        "### Data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnO8nPOvkGr9"
      },
      "outputs": [],
      "source": [
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "   index = 0\n",
        "   # This loop will keep generating training data indefinitely\n",
        "   while True:\n",
        "       x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n",
        "       y = np.zeros((batch_size), dtype=np.int32)\n",
        "       for i in range(batch_size):\n",
        "           for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
        "               x[i, t] = word_indices[w]\n",
        "           y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
        "           index = index + 1\n",
        "       yield x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P6bGypUkGr9"
      },
      "outputs": [],
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "   # helper function to sample an index from a probability array\n",
        "   preds = np.asarray(preds).astype('float64')\n",
        "   preds = np.log(preds) / temperature\n",
        "   exp_preds = np.exp(preds)\n",
        "   preds = exp_preds / np.sum(exp_preds)\n",
        "   probas = np.random.multinomial(1, preds, 1)\n",
        "   return np.argmax(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrA0ri3HkGr9"
      },
      "outputs": [],
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "   # Function invoked at end of each epoch. Prints generated text.\n",
        "   examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
        "   # Randomly pick a seed sequence\n",
        "   seed_index = np.random.randint(len(X_train+X_test))\n",
        "   seed = (X_train+X_test)[seed_index]\n",
        "\n",
        "   for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "       sentence = seed\n",
        "       examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
        "       examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
        "       examples_file.write(' '.join(sentence))\n",
        "       for i in range(50):\n",
        "           x_pred = np.zeros((1, MIN_SEQ))\n",
        "           for t, word in enumerate(sentence):\n",
        "               x_pred[0, t] = word_indices[word]\n",
        "           preds = model.predict(x_pred, verbose=0)[0]\n",
        "           next_index = sample(preds, diversity)\n",
        "           next_word = indices_word[next_index]\n",
        "\n",
        "           sentence = sentence[1:]\n",
        "           sentence.append(next_word)\n",
        "\n",
        "           examples_file.write(\" \"+next_word)\n",
        "       examples_file.write('\\n')\n",
        "   examples_file.write('='*80 + '\\n')\n",
        "   examples_file.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc5MWaPMkGr-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Activation, Bidirectional\n",
        "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
        "from __future__ import print_function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lCHV1e9kGr-"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "   print('Build model...')\n",
        "   model = Sequential()\n",
        "   model.add(Embedding(input_dim=len(word_indices), output_dim=1024))\n",
        "   model.add(Bidirectional(LSTM(128)))\n",
        "   model.add(Dense(len(word_indices)))\n",
        "   model.add(Activation('softmax'))\n",
        "   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "likcmC-fkGr_"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1024\n",
        "MIN_FREQUENCY = 7\n",
        "MIN_SEQ = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPoxGkd-kGr_",
        "outputId": "9cb9b8eb-b42b-4feb-cc04-c87de1a03103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n",
            "Epoch 1/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 65ms/step - accuracy: 0.1585 - loss: 5.5323 - val_accuracy: 0.0488 - val_loss: 7.3060\n",
            "Epoch 2/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 64ms/step - accuracy: 0.2313 - loss: 4.4559 - val_accuracy: 0.0429 - val_loss: 7.7660\n",
            "Epoch 3/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.2648 - loss: 4.1029 - val_accuracy: 0.0399 - val_loss: 8.0809\n",
            "Epoch 4/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 63ms/step - accuracy: 0.2901 - loss: 3.8750 - val_accuracy: 0.0375 - val_loss: 8.3107\n",
            "Epoch 5/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 63ms/step - accuracy: 0.3108 - loss: 3.7039 - val_accuracy: 0.0367 - val_loss: 8.5351\n",
            "Epoch 6/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 64ms/step - accuracy: 0.3288 - loss: 3.5648 - val_accuracy: 0.0351 - val_loss: 8.7366\n",
            "Epoch 7/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.3443 - loss: 3.4485 - val_accuracy: 0.0345 - val_loss: 8.9178\n",
            "Epoch 8/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.3585 - loss: 3.3469 - val_accuracy: 0.0345 - val_loss: 9.0874\n",
            "Epoch 9/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.3717 - loss: 3.2591 - val_accuracy: 0.0327 - val_loss: 9.2357\n",
            "Epoch 10/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.3836 - loss: 3.1804 - val_accuracy: 0.0325 - val_loss: 9.3913\n",
            "Epoch 11/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.3951 - loss: 3.1074 - val_accuracy: 0.0330 - val_loss: 9.5316\n",
            "Epoch 12/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.4055 - loss: 3.0430 - val_accuracy: 0.0323 - val_loss: 9.6565\n",
            "Epoch 13/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.4144 - loss: 2.9864 - val_accuracy: 0.0326 - val_loss: 9.7803\n",
            "Epoch 14/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.4233 - loss: 2.9346 - val_accuracy: 0.0314 - val_loss: 9.8813\n",
            "Epoch 15/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.4315 - loss: 2.8855 - val_accuracy: 0.0305 - val_loss: 9.9776\n",
            "Epoch 16/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 64ms/step - accuracy: 0.4396 - loss: 2.8405 - val_accuracy: 0.0308 - val_loss: 10.0959\n",
            "Epoch 17/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 64ms/step - accuracy: 0.4465 - loss: 2.7990 - val_accuracy: 0.0310 - val_loss: 10.2135\n",
            "Epoch 18/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 81ms/step - accuracy: 0.4531 - loss: 2.7608 - val_accuracy: 0.0308 - val_loss: 10.2935\n",
            "Epoch 19/20\n",
            "\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 64ms/step - accuracy: 0.4594 - loss: 2.7243 - val_accuracy: 0.0304 - val_loss: 10.3897\n",
            "Epoch 20/20\n",
            "\u001b[1m2310/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.4646 - loss: 2.6944"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-670bc1600a1d>:4: RuntimeWarning: divide by zero encountered in log\n",
            "  preds = np.log(preds) / temperature\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2311/2311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 64ms/step - accuracy: 0.4646 - loss: 2.6944 - val_accuracy: 0.0294 - val_loss: 10.4757\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7faaf02eacb0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
        "           \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
        "           (len(word_indices), MIN_SEQ, MIN_FREQUENCY) + \".keras\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
        "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
        "examples_file = open('examples.txt', \"w\")\n",
        "model.fit(generator(X_train, y_train, BATCH_SIZE),\n",
        "                   steps_per_epoch=int((len(X_train)+len(X_test))/BATCH_SIZE) + 1,\n",
        "                   epochs=20,\n",
        "                   callbacks=callbacks_list,\n",
        "                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n",
        "                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lyrics_model.keras')"
      ],
      "metadata": {
        "id": "IjT1WGeVk3cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a line to check how the model is working\n",
        "\n",
        "model.evaluate(generator(X_test, y_test, BATCH_SIZE), steps=int(len(y_test)/BATCH_SIZE) + 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebafwPfJlKbJ",
        "outputId": "2ae01aff-3f8b-4af4-88ad-5517ae04ecce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 1s 20ms/step - loss: 4.0136 - accuracy: 0.3572\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.013584613800049, 0.3571517765522003]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('lyrics_model.keras')\n"
      ],
      "metadata": {
        "id": "-RXxdqU42RYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c6ea5d-586c-45f6-fcaf-2d5d9e09c2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 11 variables whereas the saved optimizer has 20 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom input line\n",
        "custom_line = \"I am just a normal guy\"\n",
        "\n",
        "# This translator is used to remove punctuation from the text\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Tokenize the custom line\n",
        "custom_line_tokens = custom_line.lower().translate(translator).split()\n",
        "\n",
        "# Convert tokens to indices\n",
        "seed = [word_indices[word] for word in custom_line_tokens if word in word_indices]\n",
        "\n",
        "# Pad the seed to match the expected input length\n",
        "while len(seed) < MIN_SEQ:\n",
        "    seed.insert(0, 0)  # Pad with zeros\n",
        "\n",
        "# Generate lyrics\n",
        "generated_lyrics = ' '.join(custom_line_tokens)\n",
        "for i in range(50):  # Generate 50 more words\n",
        "    x_pred = np.zeros((1, MIN_SEQ+1))\n",
        "    for t, word_index in enumerate(seed):\n",
        "        x_pred[0, t] = word_index\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = sample(preds, 0.5)  # Adjust diversity as needed\n",
        "    next_word = indices_word[next_index]\n",
        "    generated_lyrics += ' ' + next_word\n",
        "    seed = seed[1:]\n",
        "    seed.append(next_index)\n",
        "\n",
        "print(generated_lyrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wygEmPEN2kMZ",
        "outputId": "04a26700-a4d4-4583-8331-0364a5f5c9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am just a normal guy \n",
            " i could just dial the phone now \n",
            " just a girl like you \n",
            " and i like the way i want it \n",
            " yeah yeah yeah \n",
            " i got a ticket for a little while she goes \n",
            " a little bit of pain left me \n",
            " and i cant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cells to run"
      ],
      "metadata": {
        "id": "e6yj0QaT-HxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5vdlk8I-P1x",
        "outputId": "d9203397-461c-493b-ab42-103f2929715c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string"
      ],
      "metadata": {
        "id": "l2ZzidMW-jbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This translator is used to remove punctuation from the text\n",
        "translator = str.maketrans('', '', string.punctuation)"
      ],
      "metadata": {
        "id": "4yl1fSgE-5O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "lQakOkO0_Esm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/data.pkl', 'rb'))\n",
        "\n",
        "X_train = data['X_train']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']\n",
        "word_indices = data['word_indices']\n",
        "indices_word = data['indices_word']"
      ],
      "metadata": {
        "id": "yvxM5SOZ_IYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "MIN_FREQUENCY = 7\n",
        "MIN_SEQ = 5"
      ],
      "metadata": {
        "id": "qL_WcjRF_OFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "u-FaSp0R_z0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/lyrics_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd0P2QV2_RvC",
        "outputId": "16a682d9-9beb-403f-d8eb-595b8a6813cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 11 variables whereas the saved optimizer has 20 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "   # helper function to sample an index from a probability array\n",
        "   preds = np.asarray(preds).astype('float64')\n",
        "   preds = np.log(preds) / temperature\n",
        "   exp_preds = np.exp(preds)\n",
        "   preds = exp_preds / np.sum(exp_preds)\n",
        "   probas = np.random.multinomial(1, preds, 1)\n",
        "   return np.argmax(probas)"
      ],
      "metadata": {
        "id": "CmfcS9McSjNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom input line\n",
        "custom_line = \"I am just a normal guy\"\n",
        "\n",
        "# This translator is used to remove punctuation from the text\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Tokenize the custom line\n",
        "custom_line_tokens = custom_line.lower().translate(translator).split()\n",
        "\n",
        "# Convert tokens to indices\n",
        "seed = [word_indices[word] for word in custom_line_tokens if word in word_indices]\n",
        "\n",
        "# Pad the seed to match the expected input length\n",
        "while len(seed) < MIN_SEQ:\n",
        "    seed.insert(0, 0)  # Pad with zeros\n",
        "\n",
        "# Generate lyrics\n",
        "generated_lyrics = ' '.join(custom_line_tokens)\n",
        "for i in range(50):  # Generate 50 more words\n",
        "    x_pred = np.zeros((1, MIN_SEQ+1))\n",
        "    for t, word_index in enumerate(seed):\n",
        "        x_pred[0, t] = word_index\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = sample(preds, 0.1)  # Adjust diversity as needed\n",
        "    next_word = indices_word[next_index]\n",
        "    generated_lyrics += ' ' + next_word\n",
        "    seed = seed[1:]\n",
        "    seed.append(next_index)\n",
        "\n",
        "print(generated_lyrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_9XHDpU_VLh",
        "outputId": "748ab3ee-02b2-4b71-efa8-c7561078e73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am just a normal guy \n",
            " i just want to think about you \n",
            " i know you know i love it \n",
            " i dont want to lose your mind \n",
            " but i dont know why i dont even know you \n",
            " but i know that i aint never been \n",
            " i aint the way it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0obH1ENESdQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}